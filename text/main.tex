\documentclass[11pt]{report}

% font
\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{IPAexMincho}
\usepackage{xeCJK}
\setCJKmainfont{IPAexMincho}

% chapter
\usepackage{titlesec}
\titleformat{\chapter}[hang]
	{\normalfont\LARGE\sffamily\bfseries}{第 \thechapter 章}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3 ex plus .2ex}

% title
\usepackage{titling}
\newcommand{\subtitle}[1]{%
	\posttitle{%
	\par\end{center}
	\begin{center}\large#1\end{center}
	\vskip0.5em}%
}
\title{架空世界創作のための言語モデル}
\subtitle{〜統計と計算を言語に組み込む創作のあり方を模索する〜}
\author{@nymwa}
\date{2020/**/**}

% graphic
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{colortbl}

% tikz
\usepackage{tikz}
\usetikzlibrary{automata}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{intersections, calc}
\usetikzlibrary{decorations}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
\usetikzlibrary{fit}
\usetikzlibrary{math}
\usetikzlibrary{shapes}

% href言語モデル
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,
	urlcolor=cyan,
	pdfnewwindow=true}
\newcommand{\link}[2]{{\footnotesize (\href{#2}{#1})}}

% math
\usepackage{amsmath,amssymb,amsthm}
\newtheorem{theorem}{定理}
\newtheorem{definition}[theorem]{定義}
\newtheorem{axiom}[theorem]{公理}
\usepackage{bm}

% listings
\usepackage{listings}
\lstset{
	backgroundcolor = {\color[rgb]{0,0,0}},
	basicstyle      = {\color[rgb]{1.0, 1.0, 1.0} \small\ttfamily},
	identifierstyle = {\color[rgb]{0.7, 0.7, 1.0} \small},
	commentstyle    = {\color[rgb]{1.0, 0.5, 0.0} \small\ttfamily},
	keywordstyle    = {\color[rgb]{1.0, 1.0, 0.0} \small\bfseries},
	ndkeywordstyle  = {\color[rgb]{0.8, 0.8, 0.8} \small},
	stringstyle     = {\color[rgb]{0.0, 1.0, 0.0} \small\ttfamily},
	frame           = {tlb},
	framesep        = 1pt,
	breaklines      = true,
	columns         = [l]{fullflexible},
	xrightmargin    = 20pt,
	xleftmargin     = 20pt,
	morecomment=[l]{//}
}

% other
\usepackage{caption}
\usepackage{cancel}
\usepackage{epigraph}
\usepackage{fancybox}
\usepackage{makecell}

\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

\renewcommand{\figurename}{図}
\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits}
\newcommand{\cnt}{\mathop{\rm count}\limits}

\begin{document}

\maketitle

\renewcommand{\contentsname}{目次}
\tableofcontents

\chapter{はじめに}

私は架空世界における言語創作，一般に人工言語と呼ばれているものと，
計算機で言語を扱う学問，理学的には計算言語学，工学的には自然言語処理と呼ばれているものが好きです．
そのような立場として，架空世界における言語創作に計算言語学的な知見が応用できる可能性があるのかと考えることがあります．

当然指輪物語が書かれた時代に計算機はなかったですし，
計算機は言語創作においては必ず必要なものではないかもしれません．
しかし，計算言語学の知見はかな漢字変換や綴り誤り訂正，
機械翻訳や対話システムなど身の回りにある多くのツールに活かされており，
その成果を人工言語へ適用すれば，
語彙や例文が微々たる架空言語に対してもそのような便利なツールが製作できる可能性があります．
言語創作に計算機を簡単に応用できるようなツールが多くの人によって作られ使われれば
創作者だけでなく，学習者にとっても利点があるものと信じています．

一方で，計算言語学や自然言語処理の発展も，そのほとんどが高々ここ半世紀のうちに成し遂げられたものに過ぎず，
昨今の機械翻訳や対話システムが人間から見て不自然な挙動をする事例からも明らかなように，
現時点では計算機で言語を扱うことは非常に難しく，また，高度な処理を行うためにはたびたび大規模なデータセットと計算機資源が必要になります．
架空言語の創作は大規模なデータを供給することが困難ですし，すべての人が高価な計算機資源を利用できるわけではなく，このような状況は必ずしも好ましいものではありません．

そのため，今回はCPUが１つあればできるような軽量な計算で実現できる古典的な手法のみを用いて
計算と統計によって言語創作に有益なツールが作れるのかどうかを模索することに焦点を置いています．
特に，簡単に実装・実用が可能な言語モデルと呼ばれるものを用いた応用について検討していきます．

この文書によって人工言語界隈に前よりちょっとだけ計算言語学が普及して
いい感じなツールが生まれてくれば嬉しいです．

本書で使用するプログラムはすべてpython 3.8で書かれます．
実行環境は標準的なUNIX/linux環境を想定しています．
なんか動かなかったりよくわからない場合は twitter:@nymwa に文句を言ってください．なんか答えます．

\chapter{言語モデルと確率}

以下の２文を見比べてみてください．
\begin{itemize}
	\item 色を着けてニスを塗った．
	\item 色を着けテニスを塗った．
\end{itemize}
最初の文が自然な文であるのに対し，２番目の文は意味をなさない不自然な文となっています．
この２文は日本語の話者であればどちらが自然か，不自然かは容易に見分けられます．
「テニス」と「塗る」はふつう共起しないことからも後者の文が不自然なことが説明できます．

しかし，かな漢字変換システムでは後者が最初に候補として示されることもあるかもしれません．
これは計算機にとっては人間にとっての文の自然さ・不自然さを理解することが容易ではないためです．
計算機と人間の構造が違っているのだからこれはしょうがないことではあるのですが，
裏を返せば，計算機に文の自然さを判定させられるようになれば，
かな漢字変換や綴り誤り訂正システムなどの便利なソフトウェアが作れるということでもあります．

ここで，理想的に世の中のすべての文に対して，その文が出現する確率を考えることにします．
ここでいう確率は，その文の会話での現れやすさ・テキスト中での起こりやすさのことであると考えてください．
例えば，「色を着けてニスを塗った．」の確率は
\begin{equation*}
	P(\mathrm{色を着けてニスを塗った．})
\end{equation*}
と書けます．
自然な文や流暢な文は，不自然な文やぎこちない文よりも世の中のすべての文全体での出現頻度が高そうです．
すると，「色を着けてニスを塗った．」は「色を着けテニスを塗った．」よりも自然な文なので，
\begin{equation*}
	P(\mathrm{色を着けてニスを塗った．})
	>
	P(\mathrm{色を着けテニスを塗った．})
\end{equation*}
となるはずです．
このようにすれば，確率の計算によって自然な文と不自然な文を識別できそうです．

文を確率変数とする確率分布のことを言語モデルと言います．
より簡単に表現するならば，言語モデルは文に対してその確率を計算することができるものです．
言語モデルはテキスト中によく出てくる文に高い確率を割り当て，
そうでない文に対しては低い確率を割り当てる必要があります．
この章では言語モデルをどのように設計すればいいかについて，基礎的な事項を説明していきます．

\section{ちょっと確率統計の復習}

言語モデルを理解するためには確率と統計の基本的な知識が必要です．
用語の使い方を明確にするためにも，最初に確率と統計の基礎的な事項について説明します
\footnote{高校数学並のいい加減な説明だけど本論に影響しないので許して．}．
条件付き確率とか期待値とかわかってる人にとっては当たり前の内容なので，節\ref{sec:1gramLM}まで読み飛ばしてもらってもいいと思います．

\subsection{確率・結果・事象}

\begin{definition}[試行と結果]
	実験や観測によって偶然に決まる事柄を結果と言い，
	その結果が偶然に決まった実験や観測のことを試行と言います．
\end{definition}

サイコロを振って出た目はその結果であり，
出た目は偶然に決まるので，サイコロを振る行為は試行と言えます．
言語モデルではあるひとつの文の観測を試行とみなし，その文を結果とします．

\begin{definition}[標本点と標本空間]
	一回の試行の結果として起こりうるものを標本点と言い，すべての標本点からなる集合を標本空間と言います．
	標本空間はギリシャ文字$\Omega$で表します．
\end{definition}

例えば，６面のサイコロを振って出た目を結果とする場合，標本点は$1,2,3,4,5,6$のいずれかで，標本空間$\Omega$は$\Omega = \{1,2,3,4,5,6\}$です．
言語モデルの場合は標本点は文なので，標本空間は可能なすべての文です．言語モデルの標本空間は，自然言語では無限集合になるはずです．
多くの人工言語でも無限集合になるはずだと思います
\footnote{節の再帰ができる言語ではいくらでも長い文が作れ，その標本空間は無限集合になります．
詳しくは"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo."で検索してください．
また，文の標本空間が有限集合となるためには節の再帰ができないことが必要です．}．

確率は標本空間のそれぞれの標本点に割り当てられる実数値です．
この値は標本空間全体での起こりやすさを$1$とした場合のその標本点の起こりやすさを表します．
６面のサイコロを振って１が出る確率は，目の出やすさに偏りがない場合６回に１回ぐらい１が出るため，$\frac{1}{6}$です．

標本点の確率は以下の確率の公理を満たすものとします．

\begin{axiom}[確率の公理]
	標本空間$\Omega$のそれぞれの標本点$x$の確率$P(x)$は以下の制約を満たします．
	\begin{enumerate}
		\item すべての$x \in \Omega$に対して，$0 \leq P(x) \leq 1$です．
		\item 標本空間のすべての標本点の確率を足し合わせた和は$1$です．すなわち，$\displaystyle \sum_{x \in \Omega} P(x) = 1$．
	\end{enumerate}
\end{axiom}

標本点の確率しか求められないと不便なので，標本点の集合の確率も定義します．

\begin{definition}[事象]
	標本空間の部分集合のことを事象といいます.
\end{definition}

６面サイコロを降って２以下の目が出る，偶数の目が出るなどが事象の例です．

事象の$E$確率は
\begin{equation*}
	P(E) = \sum_{x \in E} P(x)
\end{equation*}
です．
６面サイコロを降って２以下の目が出る事象の確率は$\frac{1}{3}$です．

\subsection{確率変数・確率分布・同時確率}

確率の議論をするときに，「もしもこの試行の結果が○○だったら...」という仮定を置くたいこともあるでしょう．
そんなときに，「この試行の結果を$X$とする」というふうに結果を置いておく変数を確率変数と言います．

\begin{definition}[確率変数]
	その変数がとる各値に対してそれぞれ確率が与えられている変数を確率変数と言います．
\end{definition}

確率変数は普通大文字で書かれ，確率変数の値は小文字で書かれます．
例えば，サイコロを振って出る目$X$は確率変数です．
$X=1$となる確率，すなわち，１の目が出る確率は，$P(X=1)=\frac{1}{6}$と書けます．

確率変数を使えば事象の確率を表現することもできます．例えばサイコロを振った目$X$が２以下となる確率は$P(X \leq 2)$と表されます．

\begin{definition}[確率分布]
	$f(x) = P(X=x)$を$X$の確率分布と言います．
\end{definition}

確率分布は確率の重みがどのように分布しているかを表します．
さいころを振って出る目の確率ははすべての目で等しいことになっているので，
図\ref{fig:dice1}のように一様な確率分布で表現されます．
２つのさいころを振って出る目の和の確率は
図\ref{fig:dice2}のように$X=7$が最も頻度が高い確率分布になっていることがわかります．

\begin{figure}[ht]
	\begin{minipage}[b]{0.5\hsize}
		\centering
		\begin{tikzpicture}[scale=0.8]
			\draw[->] (0, 0) -- (7, 0);
			\draw (1, 0) -- (1, 1);
			\draw (2, 0) -- (2, 1);
			\draw (3, 0) -- (3, 1);
			\draw (4, 0) -- (4, 1);
			\draw (5, 0) -- (5, 1);
			\draw (6, 0) -- (6, 1);
			\node at (1, -0.5) {\small 1};
			\node at (2, -0.5) {\small 2};
			\node at (3, -0.5) {\small 3};
			\node at (4, -0.5) {\small 4};
			\node at (5, -0.5) {\small 5};
			\node at (6, -0.5) {\small 6};
			\node at (0.5, 0.5) {$\frac{1}{6}$};
		\end{tikzpicture}
		\caption{\small さいころの目の確率}
		\label{fig:dice1}
	\end{minipage}
	\begin{minipage}[b]{0.5\hsize}
		\centering
		\begin{tikzpicture}[scale=0.8]
			\draw[->] (0, 0) -- (7, 0);
			\draw (1.0, 0) -- (1.0, 1/3);
			\draw (1.5, 0) -- (1.5, 2/3);
			\draw (2.0, 0) -- (2.0, 1);
			\draw (2.5, 0) -- (2.5, 4/3);
			\draw (3.0, 0) -- (3.0, 5/3);
			\draw (3.5, 0) -- (3.5, 2);
			\draw (4.0, 0) -- (4.0, 5/3);
			\draw (4.5, 0) -- (4.5, 4/3);
			\draw (5.0, 0) -- (5.0, 1);
			\draw (5.5, 0) -- (5.5, 2/3);
			\draw (6.0, 0) -- (6.0, 1/3);
			\node at (1.0, -0.5) {\scriptsize 2};
			\node at (1.5, -0.5) {\scriptsize 3};
			\node at (2.0, -0.5) {\scriptsize 4};
			\node at (2.5, -0.5) {\scriptsize 5};
			\node at (3.0, -0.5) {\scriptsize 6};
			\node at (3.5, -0.5) {\scriptsize 7};
			\node at (4.0, -0.5) {\scriptsize 8};
			\node at (4.5, -0.5) {\scriptsize 9};
			\node at (5.0, -0.5) {\scriptsize 10};
			\node at (5.5, -0.5) {\scriptsize 11};
			\node at (6.0, -0.5) {\scriptsize 12};
			\node at (0.5, 0.5) {$\frac{1}{36}$};
			\node at (3.5, 2.5) {$\frac{1}{6}$};
		\end{tikzpicture}
		\caption{\small ２個のさいころの目の和の確率}
		\label{fig:dice2}
	\end{minipage}
\end{figure}

\begin{definition}[同時確率]
	$n$個の確率変数$X_1, X_2, \cdots, X_n$がそれぞれある値をとる時の確率$P(X_1, X_2, \cdots, X_n)$を同時確率と言います．
\end{definition}

さいころ１，さいころ２の出る目がそれぞれ$X_1, X_2$としたとき，$X_1=1, X_2=2$となるる確率は同時確率で，
\begin{equation*}
	P(X_1 = 1, X_2 = 2) = \cfrac{1}{36}
\end{equation*}
と書けます．

\subsection{条件付き確率と周辺確率}

英単語ではほとんどの単語でqの直後にはuが来ることが知られています．
このことから，英単語では１文字目がqであるとき２文字目がuである確率はほぼ1に近いと言えます\footnote{アラブ圏の地名が多く出てくる文章とかだと違いそうですが．}．
これは，
英単語の１文字目を$X_1$，２文字目を$X_2$としたとき，
$X_1 = \mathrm{q}$
であることがすでにわかっている場合に
$X_2 = \mathrm{u}$
となる確率が$1$に近いということです．

\begin{definition}[条件付き確率]
	事象$B$が起きたとわかっている場合に事象$A$が起きる確率を
	$B$を条件とする$A$の条件付き確率と言い，
	$P(A|B)$で表す．
\end{definition}

つまり，$P(X_2=\mathrm{u}|X_1=\mathrm{q}) \simeq 1$と書けます．

条件付き確率$P(A|B)$は
\begin{equation*}
	P(A|B) = \cfrac{P(A, B)}{P(B)} \: (\text{ただし，}P(B)>0)
\end{equation*}
と定義されます．$P(B)=0$のときは$B$を条件とする条件付き確率は定義されません．

複数の確率変数があるとき，それぞれの確率変数の確率分布のことを周辺確率分布と言います．
条件付き確率の分子は同時確率で，分母は周辺確率になっていると言えます．

\subsection{独立}

\begin{definition}[独立性]
	hoge
\end{definition}

\subsection{期待値}

\begin{definition}[確率変数の期待値]
	hoge
\end{definition}

\section{1-gram言語モデル}\label{sec:1gramLM}

\section{言語モデルの評価}

\section{n-gram言語モデル}

\section{Kneser-Ney スムージング}

\section{RNN言語モデル}

\chapter{トキポナ言語モデルを作る}

\section{トキポナとは}

\section{言語モデルの学習}

\section{ドメイン適応}

\section{長さ正規化}

\chapter{造語支援システムを作る}

\section{言語モデルによる生成}

\section{ビーム探索}

\section{top-p サンプリング}

\chapter{綴り誤り訂正システムを作る}

\section{雑音チャネルモデル}

\section{実世界の誤りへの適応}

\section{エンコーダーデコーダーモデル}

\section{エンコーダーデコーダーモデルによる綴り誤り訂正}

\chapter{トキポナ誤り訂正システムを作る}

\section{雑音チャネルモデルによる定式化}

\section{アライメントモデル}

\section{アライメントモデルの学習と推論}

\section{句に基づく翻訳モデル}

\section{句翻訳モデル}

\section{句歪みモデル}

\section{デコーダー}

\section{句に基づく翻訳モデルによるトキポナ誤り訂正}

\section{エンコーダーデコーダーモデルによるトキポナ誤り訂正}

\chapter{おわりに}

\appendix
\def\thesection{補遺\Alph{section}}

\chapter{python 3.8のインストール}

\chapter{python仮想環境のインストール}

\chapter{本書サンプルコードのダウンロードと実行}

\end{document}

